
# wayback_machine_downloader

## 小坑

下載有時會出現。
```
Failed to open TCP connection to web.archive.org:443 (Connection refused 
```

解決辦法
```
https://github.com/hartator/wayback-machine-downloader/issues/267
```

切換目錄找檔案:
```
/var/lib/gems/3.1.0/gems/wayback_machine_downloader-2.3.1/lib
```

### 照這個對比手動修改，(我沒研究過自動修改)
```
diff --git a/lib/wayback_machine_downloader.rb b/lib/wayback_machine_downloader.rb
index 730714a..199b9dd 100644
--- a/lib/wayback_machine_downloader.rb
+++ b/lib/wayback_machine_downloader.rb
@@ -206,11 +206,15 @@ class WaybackMachineDownloader
     @processed_file_count = 0
     @threads_count = 1 unless @threads_count != 0
     @threads_count.times do
+      http = Net::HTTP.new("web.archive.org", 443)
+      http.use_ssl = true
+      http.start()
       threads << Thread.new do
         until file_queue.empty?
           file_remote_info = file_queue.pop(true) rescue nil
-          download_file(file_remote_info) if file_remote_info
+          download_file(file_remote_info, http) if file_remote_info
         end
+        http.finish()
       end
     end

@@ -243,7 +247,7 @@ class WaybackMachineDownloader
     end
   end

-  def download_file file_remote_info
+  def download_file (file_remote_info, http)
     current_encoding = "".encoding
     file_url = file_remote_info[:file_url].encode(current_encoding)
     file_id = file_remote_info[:file_id]
@@ -268,8 +272,8 @@ class WaybackMachineDownloader
         structure_dir_path dir_path
         open(file_path, "wb") do |file|
           begin
-            URI("https://web.archive.org/web/#{file_timestamp}id_/#{file_url}").open("Accept-Encoding" => "plain") do |uri|
-              file.write(uri.read)
+            http.get(URI("https://web.archive.org/web/#{file_timestamp}id_/#{file_url}")) do |body|
+              file.write(body)
             end
           rescue OpenURI::HTTPError => e
             puts "#{file_url} # #{e}"
```

### 在wayback_machine_downloader.rb
```
  def get_all_snapshots_to_consider
    # Note: Passing a page index parameter allow us to get more snapshots,
    # but from a less fresh index
    http = Net::HTTP.new("web.archive.org", 443)
    http.use_ssl = true
    http.start()
    print "Getting snapshot pages"
    snapshot_list_to_consider = []
    snapshot_list_to_consider += get_raw_list_from_api(@base_url, nil, http)
    print "."
    unless @exact_url
      @maximum_pages.times do |page_index|
        snapshot_list = get_raw_list_from_api(@base_url + '/*', page_index, http)
        break if snapshot_list.empty?
        snapshot_list_to_consider += snapshot_list
        print "."
      end
    end
    http.finish()
    puts " found #{snapshot_list_to_consider.length} snaphots to consider."
    puts
    snapshot_list_to_consider
  end
```


### ……並且在archive_api.rb

```
  def get_raw_list_from_api url, page_index, http
    request_url = URI("https://web.archive.org/cdx/search/xd")
    params = [["output", "json"], ["url", url]]
    params += parameters_for_api page_index
    request_url.query = URI.encode_www_form(params)

    begin
      json = JSON.parse(http.get(URI(request_url)).body)
      if (json[0] <=> ["timestamp","original"]) == 0
        json.shift
      end
      json
    rescue JSON::ParserError
      []
    end
  end
```





## 參數
```
Optional options:
    -d, --directory PATH             Directory to save the downloaded files into
                                     Default is ./websites/ plus the domain name
    -s, --all-timestamps             Download all snapshots/timestamps for a given website
    -f, --from TIMESTAMP             Only files on or after timestamp supplied (ie. 20060716231334)
    -t, --to TIMESTAMP               Only files on or before timestamp supplied (ie. 20100916231334)
    -e, --exact-url                  Download only the url provied and not the full site
    -o, --only ONLY_FILTER           Restrict downloading to urls that match this filter
                                     (use // notation for the filter to be treated as a regex)
    -x, --exclude EXCLUDE_FILTER     Skip downloading of urls that match this filter
                                     (use // notation for the filter to be treated as a regex)
    -a, --all                        Expand downloading to error files (40x and 50x) and redirections (30x)
    -c, --concurrency NUMBER         Number of multiple files to download at a time
                                     Default is one file at a time (ie. 20)
    -p, --maximum-snapshot NUMBER    Maximum snapshot pages to consider (Default is 100)
                                     Count an average of 150,000 snapshots per page
    -l, --list                       Only list file urls in a JSON format with the archived timestamps, won't download anything
    -v, --version                    Display version

```



```
wayback_machine_downloader dmoe.org

wayback_machine_downloader dmoe.org -a

wayback_machine_downloader dmoe.org --all-timestamps 


wayback_machine_downloader dmoe.org --only "/\.(gif|jpg|jpeg|icon)$/i"


wayback_machine_downloader dmoe.org --all-timestamps  -a



//最好用 # 建議搭配sublime    subl ./
wayback_machine_downloader dmoe.org --all-timestamps   -p 500  -a

//最好用 不下載錯誤或跳轉 (40x and 50x) and redirections (30x)
wayback_machine_downloader xcy.plus --all-timestamps   -p 500  





//這個是目錄方式呈現，但會有錯誤，舊站跟新站目錄混在一起，甚至會覆蓋，所以不全面。適合快速url， 這功能就是 wayback 點URL的選單的URL
wayback_machine_downloader dmoe.org    -a       
```








## 抽取子域，或是其他頂級域名的腳本演變過程
```
awk '{
    while (match($0, /[a-zA-Z0-9.-]+\.?dmoe\.[a-zA-Z0-9.-]+/)) {
        print substr($0, RSTART, RLENGTH)
        $0 = substr($0, RSTART + RLENGTH)
    }
}' index.html


------------------------------------------------------

find /home/kali/Desktop/websites/dmoe.us -type f -name "*.html" -exec awk '{
    while (match($0, /[a-zA-Z0-9.-]+\.?dmoe\.[a-zA-Z0-9.-]+/)) {
        print substr($0, RSTART, RLENGTH)
        $0 = substr($0, RSTART + RLENGTH)
    }
}' {} +

------------------------------------------------------



find /home/kali/Desktop/websites/dmoe.us -type f -name "*.html" -exec awk '{
    while (match($0, /[a-zA-Z0-9.-]+\.?dmoe\.[a-zA-Z0-9.-]+/)) {
        print substr($0, RSTART, RLENGTH)
        $0 = substr($0, RSTART + RLENGTH)
    }
}' {} + | sort | uniq



-------------------------------
#!/bin/bash

# 檢查是否提供了足夠的參數
if [ "$#" -ne 2 ]; then
    echo "Usage: $0 <directory_path> <domain>"
    exit 1
fi

# 使用提供的目錄路徑和目標域名
directory_path="$1"
target_domain="$2"

find "$directory_path" -type f -name "*.html" -exec awk -v target="$target_domain" '{
    while (match($0, "[a-zA-Z0-9.-]+\\.?" target "\\.[a-zA-Z0-9.-]+")) {
        print substr($0, RSTART, RLENGTH)
        $0 = substr($0, RSTART + RLENGTH)
    }
}' {} + | sort | uniq

------------------------------------------------------

# 使用方法
./extract_domains.sh /home/kali/Desktop/websites/ dmoe

2Fwww.dmoe.in
i.dmoe.org
img.dmoe.org
pic.dmoe.in
stats.dmoe.moe
www.dmoe.in
.www.dmoe.moe
www.dmoe.moe
www.dmoe.org


```


驗證檢查抽取的結果，因為我url編碼問題沒處理好，所以雙重確認

```
for domain in $(cat domains.txt); do grep -l -r -w "$domain" /home/kali/Desktop/websites/ | xargs -I {} echo "域名 $domain 在文件 {} 中"; done

# 結果
域名 2Fwww.dmoe.in 在文件 /home/kali/Desktop/websites/dmoe.org/20201024001045/index.html 中
域名 2Fwww.dmoe.in 在文件 /home/kali/Desktop/websites/dmoe.in/20201021212412/index.html 中
域名 i.dmoe.org 在文件 /home/kali/Desktop/websites/dmoe.org/20150816112527/index.html 中
域名 i.dmoe.org 在文件 /home/kali/Desktop/websites/dmoe.org/20150801085842/index.html 中
域名 img.dmoe.org 在文件 /home/kali/Desktop/websites/dmoe.org/20160518040957/index.html 中
域名 img.dmoe.org 在文件 /home/kali/Desktop/websites/dmoe.org/20160407140555/index.html 中
域名 pic.dmoe.in 在文件 /home/kali/Desktop/websites/dmoe.us/20240303193538/index.html 中


```

定位取字串，所以 2fwww 是錯誤的。
```
cat /home/kali/Desktop/websites/dmoe.org/20201024001045/index.html | grep "2Fwww"


https%3A%2F%2Fwww.dmoe.in%2Fopen%3Ftype%3Dweibo
https://www.dmoe.in/open?type=weibo
```




## 心得


有時舊站會跳到最新的站

有時舊站會跳舊站，所以多使用 -a 配至，取得想要的內容。
